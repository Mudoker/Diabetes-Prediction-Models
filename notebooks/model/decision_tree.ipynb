{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "### COSC2753 - Machine Learning\n",
    "\n",
    "# **Decision Tree**\n",
    "\n",
    "<center>────────────────────────────</center>\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────┐\n",
      "│  Checking Package Versions...  │\n",
      "└────────────────────────────────┘\n",
      ">>> numpy is up to date: 1.26.4\n",
      ">>> pandas is up to date: 2.2.1\n",
      ">>> seaborn is up to date: 0.13.2\n",
      ">>> matplotlib is up to date: 3.8.3\n",
      ">>> tabulate is up to date: 0.9.0\n",
      ">>> sklearn is up to date: 1.4.1.post1\n",
      ">>> statsmodels is up to date: 0.14.1\n",
      ">>> imblearn is up to date: 0.12.2\n",
      "\u001b[1m\u001b[3m\n",
      "Done checking packages version...\n",
      "\u001b[0m\n",
      "┌───────────────────────────┐\n",
      "│  Initializing Project...  │\n",
      "└───────────────────────────┘\n",
      "\n",
      "    /\\_____/\\\n",
      "   /  x   o  \\\n",
      "  ( ==  ^  == )       Neko has arrived!\n",
      "   )         (        An data visualizing extension for analyzing DataFrames.\n",
      "  (           )       Art: https://www.asciiart.eu/animals/cats.\n",
      " ( (  )   (  ) )\n",
      "(__(__)___(__)__)\n",
      "\n",
      "\u001b[1m\u001b[3mDone initializing project...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import statsmodels\n",
    "import imblearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    GridSearchCV,\n",
    "    HalvingGridSearchCV,\n",
    ")\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "import warnings\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Reload modules\n",
    "sys.path.append(\"../../\")  # Root directory\n",
    "modules_to_reload = [\n",
    "    \"scripts.styler\",\n",
    "    \"scripts.neko\",\n",
    "    \"scripts.utils\",\n",
    "]\n",
    "\n",
    "# Reload modules if they have been modified\n",
    "missing_modules = []\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    else:\n",
    "        missing_modules.append(module_name)\n",
    "\n",
    "# Recache missing modules\n",
    "if missing_modules:\n",
    "    print(f\"Modules {missing_modules} not found. \\nRecaching...\")\n",
    "\n",
    "# Import user-defined scripts\n",
    "from scripts.styler import Styler\n",
    "from scripts.neko import Neko\n",
    "from scripts.utils import Utils\n",
    "\n",
    "\n",
    "# Initialize styler\n",
    "styler = Styler()  # Text Styler\n",
    "\n",
    "# Check package versions\n",
    "styler.draw_box(\"Checking Package Versions...\")\n",
    "\n",
    "try:\n",
    "    with open(\"../../requirements.txt\", \"r\") as file:\n",
    "        requirements = file.readlines()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '../../requirements.txt' not found.\")\n",
    "\n",
    "packages_to_check = [np, pd, sns, matplotlib, tabulate, sklearn, statsmodels, imblearn]\n",
    "\n",
    "for package in packages_to_check:\n",
    "    Utils.version_check(package, requirements=requirements)\n",
    "\n",
    "styled_text = styler.style(\n",
    "    \"\\nDone checking packages version...\\n\", bold=True, italic=True\n",
    ")\n",
    "print(styled_text)\n",
    "\n",
    "# Initialize objects\n",
    "styler.draw_box(\"Initializing Project...\")\n",
    "neko = Neko()  # Panda extension\n",
    "bullet = \">>>\"  # Bullet point\n",
    "plt = matplotlib.pyplot  # Matplotlib\n",
    "\n",
    "# Configuration\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "styled_text = styler.style(\"Done initializing project...\", bold=True, italic=True)\n",
    "print(styled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────┐\n",
      "│  Data Loaded Successfully  │\n",
      "└────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load data\n",
    "    df_train = pd.read_csv(\"../../data/processed/data_train_processed.csv\")\n",
    "    df_test = pd.read_csv(\"../../data/test/data_test.csv\")\n",
    "\n",
    "    styler.draw_box(\"Data Loaded Successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to techniques like *Logistic Regression*, a key advantage of tree-based methods is their inherent **insensitivity** to the scaling of input features. Unlike other methods, tree-based algorithms can directly utilize the data without requiring prior scaling to a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df_train.drop(columns=[\"Status\"], axis=1)\n",
    "y = df_train[\"Status\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation (First Attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Training Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28473\n",
      "           1       1.00      1.00      1.00     28536\n",
      "\n",
      "    accuracy                           1.00     57009\n",
      "   macro avg       1.00      1.00      1.00     57009\n",
      "weighted avg       1.00      1.00      1.00     57009\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82      7158\n",
      "           1       0.81      0.83      0.82      7095\n",
      "\n",
      "    accuracy                           0.82     14253\n",
      "   macro avg       0.82      0.82      0.82     14253\n",
      "weighted avg       0.82      0.82      0.82     14253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "neko.evaluate_model(dt, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous analysis, the filtering method identified features **'MentHlth'** and **'AnyHealthcare'** as insignificant for predicting the target variable. Consequently, these features will be excluded from the dataset to improve model performance and focus on the most impactful factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────────────┐\n",
      "│  Training the model (With Reduced Features)  │\n",
      "└──────────────────────────────────────────────┘\n",
      "Classification Report for Training Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28473\n",
      "           1       1.00      1.00      1.00     28536\n",
      "\n",
      "    accuracy                           1.00     57009\n",
      "   macro avg       1.00      1.00      1.00     57009\n",
      "weighted avg       1.00      1.00      1.00     57009\n",
      "\n",
      "Classification Report for Testing Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81      7158\n",
      "           1       0.81      0.81      0.81      7095\n",
      "\n",
      "    accuracy                           0.81     14253\n",
      "   macro avg       0.81      0.81      0.81     14253\n",
      "weighted avg       0.81      0.81      0.81     14253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traing the model (With reduced Features)\n",
    "styler.draw_box(\"Training the model (With Reduced Features)\")\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Drop the specified columns from X_train and X_test\n",
    "X_train_reduced = X_train.drop(columns=[\"AnyHealthcare\", \"MentHlth\"])\n",
    "X_test_reduced = X_test.drop(columns=[\"AnyHealthcare\", \"MentHlth\"])\n",
    "\n",
    "model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Make predictions\n",
    "neko.evaluate_model(model, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Model Performance**\n",
    "\n",
    "The initial model achieved an accuracy of **82%** without any adjustments to its hyperparameters. This result is a **promising starting point** for further development. However, to maximize the model's effectiveness, additional optimization techniques will be explored.\n",
    "\n",
    "**Feature Selection Analysis**\n",
    "\n",
    "An initial exploration of feature selection was conducted by removing the features **'MentHlth'** and **'AnyHealthcare'** from the dataset. Unfortunately, this did not lead to any improvement in the model's performance. In fact, the accuracy even decreased slightly to **81%**. Consequently, these features will be retained in the dataset for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Post-Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, **post-pruning** will be applied to the decision tree. This technique selectively removes branches that contribute minimally to the model's performance on a validation set, even if they improve accuracy on the training set. Prioritizing regions where training and validation accuracy are similar focuses on generalizability and reduces the risk of overfitting.\n",
    "\n",
    "Due to its effectiveness and reduced sensitivity to hyperparameter tuning compared to **pre-pruning**, post-pruning is the chosen strategy.\n",
    "\n",
    "Given the large dataset size and the application of oversampling, **RandomizedSearchCV** will be employed for efficient hyperparameter optimization of the decision tree model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # Step 2: Compute the ccp_alphas value using cost_complexity_pruning_path()\n",
    "# clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Get size of ccp_alpha valus\n",
    "# path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "# ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# best_alpha = neko.post_pruning(\n",
    "#     clf,\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     classifier=\"decision_tree\",\n",
    "#     n_iterations=300,\n",
    "#     n_jobs=6,\n",
    "# )\n",
    "\n",
    "# # Train the model with the best alpha\n",
    "# clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
    "# neko.evaluate_model(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # Confusion matrix\n",
    "# clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # Fit the model\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Confusion matrix for training set\n",
    "# train_conf_matrix = confusion_matrix(y_train, clf.predict(X_train))\n",
    "\n",
    "# # Confusion matrix for testing set\n",
    "# test_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # Plot the confusion matrices\n",
    "# plt.figure(figsize=(20, 7))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.heatmap(train_conf_matrix, annot=True, fmt=\"d\", cmap=\"Reds\")\n",
    "# plt.title(\"Confusion Matrix - Training Set\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.heatmap(test_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.title(\"Confusion Matrix - Testing Set\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Evaluation (After Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# neko.evaluate_model(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is highly overfit, with a training accuracy of **100%** and a validation accuracy of **89%**. To address this issue, hyperparameter tuning will be performed to optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Warning Suppression\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": np.arange(3, 40, 2).tolist(),\n",
    "    \"min_samples_split\": np.arange(2, 20, 2).tolist(),\n",
    "    \"min_samples_leaf\": np.arange(1, 20, 1).tolist(),\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"ccp_alpha\": np.arange(0.0, 0.1, 0.01),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 13, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 18}\n",
      "Best accuracy: 0.8545458622595306\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "search = HalvingGridSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_weighted\",\n",
    "    n_jobs=2,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", search.best_params_)\n",
    "print(\"Best accuracy:\", search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
